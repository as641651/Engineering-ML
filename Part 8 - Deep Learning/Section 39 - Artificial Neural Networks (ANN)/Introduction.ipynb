{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Single layer neural network** with input vector **x** and output vector **y**,\n",
    "\n",
    "${\\bf y} = \\phi({\\bf Wx}) \\qquad {\\bf W} \\in \\mathbb{R}^{mxn}, \\quad {\\bf x} \\in \\mathbb{R}^{n}, \\quad {\\bf y} \\in \\mathbb{R}^{m}$\n",
    "\n",
    "Where **W** is the weight matrix and $\\Phi$ is the element-wise activation function. **W** is solved with gradient descent algorithm.\n",
    "\n",
    "Recall, when **m = 1** and $\\phi$ is the **sigmoid function** we realize an alogorithm similar to the one discussed in the **logistic regression** section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Threshold function\n",
    "\n",
    "Used for binary output variables without having to model probabilities\n",
    "\n",
    "$ \\phi(z) = \\begin{cases} 1 \\text{  if  } z \\ge 0 \\\\ 0 \\text{  if  } z \\le 0 \\end{cases}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Sigmod function\n",
    "\n",
    "Smoother transition from 0 to 1. This is useful when modelling probabilities\n",
    "\n",
    "$ \\phi( z) = \\frac{1}{1+e^{-z}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Rectifier function\n",
    "\n",
    "Used to amplify vanishing gradients in deep neural networks. Hence ReLU is commonly used as activation function in the intermediate layers. To understand why ReLU is a valuable activation function, read the papaer [*Deep sparse rectifier neural networks (2011)*](http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf)\n",
    "\n",
    "$ \\phi(z) = max(z,0)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Hyperbolic tangent (tanh)\n",
    "\n",
    "Smoother transition from -1 to 1.\n",
    "\n",
    "$ \\phi( z) = \\frac{1 - e^{-2z}}{1+e^{-2z}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an error function,\n",
    "\n",
    "$E = f(y,\\hat{y})$\n",
    "\n",
    "Where $\\hat{y}$ is the actual output. Iteratively update **W** such that the gradient of E w.r.t **W** approaches 0,\n",
    "\n",
    "$ {\\bf W} = update( {\\bf W} , \\frac{\\partial E}{\\partial \\bf W}) \\qquad {\\bf W} \\in \\mathbb{W}$\n",
    "\n",
    "Where $\\mathbb{W}$ is the set of all weight matrices in different layers of neural network. The differential is  efficiently computed using ***Back propogation algorithm***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of error functions\n",
    "\n",
    "Refer [*A list of cost functions used in neura networks, alongside applications (2015)*](https://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient descent update\n",
    "\n",
    "${\\bf W} =  {\\bf W} - \\eta \\frac{\\partial E}{\\partial \\bf W} $\n",
    "\n",
    "In **Stochiastic gradient descent**, the update is performed after computation of every (or a batch of) samples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
