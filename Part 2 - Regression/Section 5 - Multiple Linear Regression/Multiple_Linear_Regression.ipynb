{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n",
    "\n",
    "Now we have a dataset that has more than one dependent variable\n",
    "\n",
    "### 1) Get the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R&amp;D Spend</th>\n",
       "      <th>Administration</th>\n",
       "      <th>Marketing Spend</th>\n",
       "      <th>State</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>165349.20</td>\n",
       "      <td>136897.80</td>\n",
       "      <td>471784.10</td>\n",
       "      <td>New York</td>\n",
       "      <td>192261.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>162597.70</td>\n",
       "      <td>151377.59</td>\n",
       "      <td>443898.53</td>\n",
       "      <td>California</td>\n",
       "      <td>191792.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>153441.51</td>\n",
       "      <td>101145.55</td>\n",
       "      <td>407934.54</td>\n",
       "      <td>Florida</td>\n",
       "      <td>191050.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>144372.41</td>\n",
       "      <td>118671.85</td>\n",
       "      <td>383199.62</td>\n",
       "      <td>New York</td>\n",
       "      <td>182901.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>142107.34</td>\n",
       "      <td>91391.77</td>\n",
       "      <td>366168.42</td>\n",
       "      <td>Florida</td>\n",
       "      <td>166187.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>131876.90</td>\n",
       "      <td>99814.71</td>\n",
       "      <td>362861.36</td>\n",
       "      <td>New York</td>\n",
       "      <td>156991.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>134615.46</td>\n",
       "      <td>147198.87</td>\n",
       "      <td>127716.82</td>\n",
       "      <td>California</td>\n",
       "      <td>156122.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>130298.13</td>\n",
       "      <td>145530.06</td>\n",
       "      <td>323876.68</td>\n",
       "      <td>Florida</td>\n",
       "      <td>155752.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>120542.52</td>\n",
       "      <td>148718.95</td>\n",
       "      <td>311613.29</td>\n",
       "      <td>New York</td>\n",
       "      <td>152211.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>123334.88</td>\n",
       "      <td>108679.17</td>\n",
       "      <td>304981.62</td>\n",
       "      <td>California</td>\n",
       "      <td>149759.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>101913.08</td>\n",
       "      <td>110594.11</td>\n",
       "      <td>229160.95</td>\n",
       "      <td>Florida</td>\n",
       "      <td>146121.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>100671.96</td>\n",
       "      <td>91790.61</td>\n",
       "      <td>249744.55</td>\n",
       "      <td>California</td>\n",
       "      <td>144259.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>93863.75</td>\n",
       "      <td>127320.38</td>\n",
       "      <td>249839.44</td>\n",
       "      <td>Florida</td>\n",
       "      <td>141585.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>91992.39</td>\n",
       "      <td>135495.07</td>\n",
       "      <td>252664.93</td>\n",
       "      <td>California</td>\n",
       "      <td>134307.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>119943.24</td>\n",
       "      <td>156547.42</td>\n",
       "      <td>256512.92</td>\n",
       "      <td>Florida</td>\n",
       "      <td>132602.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>114523.61</td>\n",
       "      <td>122616.84</td>\n",
       "      <td>261776.23</td>\n",
       "      <td>New York</td>\n",
       "      <td>129917.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>78013.11</td>\n",
       "      <td>121597.55</td>\n",
       "      <td>264346.06</td>\n",
       "      <td>California</td>\n",
       "      <td>126992.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>94657.16</td>\n",
       "      <td>145077.58</td>\n",
       "      <td>282574.31</td>\n",
       "      <td>New York</td>\n",
       "      <td>125370.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>91749.16</td>\n",
       "      <td>114175.79</td>\n",
       "      <td>294919.57</td>\n",
       "      <td>Florida</td>\n",
       "      <td>124266.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>86419.70</td>\n",
       "      <td>153514.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>New York</td>\n",
       "      <td>122776.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>76253.86</td>\n",
       "      <td>113867.30</td>\n",
       "      <td>298664.47</td>\n",
       "      <td>California</td>\n",
       "      <td>118474.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>78389.47</td>\n",
       "      <td>153773.43</td>\n",
       "      <td>299737.29</td>\n",
       "      <td>New York</td>\n",
       "      <td>111313.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>73994.56</td>\n",
       "      <td>122782.75</td>\n",
       "      <td>303319.26</td>\n",
       "      <td>Florida</td>\n",
       "      <td>110352.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>67532.53</td>\n",
       "      <td>105751.03</td>\n",
       "      <td>304768.73</td>\n",
       "      <td>Florida</td>\n",
       "      <td>108733.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>77044.01</td>\n",
       "      <td>99281.34</td>\n",
       "      <td>140574.81</td>\n",
       "      <td>New York</td>\n",
       "      <td>108552.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>64664.71</td>\n",
       "      <td>139553.16</td>\n",
       "      <td>137962.62</td>\n",
       "      <td>California</td>\n",
       "      <td>107404.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>75328.87</td>\n",
       "      <td>144135.98</td>\n",
       "      <td>134050.07</td>\n",
       "      <td>Florida</td>\n",
       "      <td>105733.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>72107.60</td>\n",
       "      <td>127864.55</td>\n",
       "      <td>353183.81</td>\n",
       "      <td>New York</td>\n",
       "      <td>105008.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>66051.52</td>\n",
       "      <td>182645.56</td>\n",
       "      <td>118148.20</td>\n",
       "      <td>Florida</td>\n",
       "      <td>103282.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>65605.48</td>\n",
       "      <td>153032.06</td>\n",
       "      <td>107138.38</td>\n",
       "      <td>New York</td>\n",
       "      <td>101004.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>61994.48</td>\n",
       "      <td>115641.28</td>\n",
       "      <td>91131.24</td>\n",
       "      <td>Florida</td>\n",
       "      <td>99937.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>61136.38</td>\n",
       "      <td>152701.92</td>\n",
       "      <td>88218.23</td>\n",
       "      <td>New York</td>\n",
       "      <td>97483.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>63408.86</td>\n",
       "      <td>129219.61</td>\n",
       "      <td>46085.25</td>\n",
       "      <td>California</td>\n",
       "      <td>97427.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>55493.95</td>\n",
       "      <td>103057.49</td>\n",
       "      <td>214634.81</td>\n",
       "      <td>Florida</td>\n",
       "      <td>96778.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>46426.07</td>\n",
       "      <td>157693.92</td>\n",
       "      <td>210797.67</td>\n",
       "      <td>California</td>\n",
       "      <td>96712.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>46014.02</td>\n",
       "      <td>85047.44</td>\n",
       "      <td>205517.64</td>\n",
       "      <td>New York</td>\n",
       "      <td>96479.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>28663.76</td>\n",
       "      <td>127056.21</td>\n",
       "      <td>201126.82</td>\n",
       "      <td>Florida</td>\n",
       "      <td>90708.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>44069.95</td>\n",
       "      <td>51283.14</td>\n",
       "      <td>197029.42</td>\n",
       "      <td>California</td>\n",
       "      <td>89949.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>20229.59</td>\n",
       "      <td>65947.93</td>\n",
       "      <td>185265.10</td>\n",
       "      <td>New York</td>\n",
       "      <td>81229.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>38558.51</td>\n",
       "      <td>82982.09</td>\n",
       "      <td>174999.30</td>\n",
       "      <td>California</td>\n",
       "      <td>81005.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>28754.33</td>\n",
       "      <td>118546.05</td>\n",
       "      <td>172795.67</td>\n",
       "      <td>California</td>\n",
       "      <td>78239.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>27892.92</td>\n",
       "      <td>84710.77</td>\n",
       "      <td>164470.71</td>\n",
       "      <td>Florida</td>\n",
       "      <td>77798.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>23640.93</td>\n",
       "      <td>96189.63</td>\n",
       "      <td>148001.11</td>\n",
       "      <td>California</td>\n",
       "      <td>71498.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>15505.73</td>\n",
       "      <td>127382.30</td>\n",
       "      <td>35534.17</td>\n",
       "      <td>New York</td>\n",
       "      <td>69758.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>22177.74</td>\n",
       "      <td>154806.14</td>\n",
       "      <td>28334.72</td>\n",
       "      <td>California</td>\n",
       "      <td>65200.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1000.23</td>\n",
       "      <td>124153.04</td>\n",
       "      <td>1903.93</td>\n",
       "      <td>New York</td>\n",
       "      <td>64926.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1315.46</td>\n",
       "      <td>115816.21</td>\n",
       "      <td>297114.46</td>\n",
       "      <td>Florida</td>\n",
       "      <td>49490.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.00</td>\n",
       "      <td>135426.92</td>\n",
       "      <td>0.00</td>\n",
       "      <td>California</td>\n",
       "      <td>42559.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>542.05</td>\n",
       "      <td>51743.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>New York</td>\n",
       "      <td>35673.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.00</td>\n",
       "      <td>116983.80</td>\n",
       "      <td>45173.06</td>\n",
       "      <td>California</td>\n",
       "      <td>14681.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    R&D Spend  Administration  Marketing Spend       State     Profit\n",
       "0   165349.20       136897.80        471784.10    New York  192261.83\n",
       "1   162597.70       151377.59        443898.53  California  191792.06\n",
       "2   153441.51       101145.55        407934.54     Florida  191050.39\n",
       "3   144372.41       118671.85        383199.62    New York  182901.99\n",
       "4   142107.34        91391.77        366168.42     Florida  166187.94\n",
       "5   131876.90        99814.71        362861.36    New York  156991.12\n",
       "6   134615.46       147198.87        127716.82  California  156122.51\n",
       "7   130298.13       145530.06        323876.68     Florida  155752.60\n",
       "8   120542.52       148718.95        311613.29    New York  152211.77\n",
       "9   123334.88       108679.17        304981.62  California  149759.96\n",
       "10  101913.08       110594.11        229160.95     Florida  146121.95\n",
       "11  100671.96        91790.61        249744.55  California  144259.40\n",
       "12   93863.75       127320.38        249839.44     Florida  141585.52\n",
       "13   91992.39       135495.07        252664.93  California  134307.35\n",
       "14  119943.24       156547.42        256512.92     Florida  132602.65\n",
       "15  114523.61       122616.84        261776.23    New York  129917.04\n",
       "16   78013.11       121597.55        264346.06  California  126992.93\n",
       "17   94657.16       145077.58        282574.31    New York  125370.37\n",
       "18   91749.16       114175.79        294919.57     Florida  124266.90\n",
       "19   86419.70       153514.11             0.00    New York  122776.86\n",
       "20   76253.86       113867.30        298664.47  California  118474.03\n",
       "21   78389.47       153773.43        299737.29    New York  111313.02\n",
       "22   73994.56       122782.75        303319.26     Florida  110352.25\n",
       "23   67532.53       105751.03        304768.73     Florida  108733.99\n",
       "24   77044.01        99281.34        140574.81    New York  108552.04\n",
       "25   64664.71       139553.16        137962.62  California  107404.34\n",
       "26   75328.87       144135.98        134050.07     Florida  105733.54\n",
       "27   72107.60       127864.55        353183.81    New York  105008.31\n",
       "28   66051.52       182645.56        118148.20     Florida  103282.38\n",
       "29   65605.48       153032.06        107138.38    New York  101004.64\n",
       "30   61994.48       115641.28         91131.24     Florida   99937.59\n",
       "31   61136.38       152701.92         88218.23    New York   97483.56\n",
       "32   63408.86       129219.61         46085.25  California   97427.84\n",
       "33   55493.95       103057.49        214634.81     Florida   96778.92\n",
       "34   46426.07       157693.92        210797.67  California   96712.80\n",
       "35   46014.02        85047.44        205517.64    New York   96479.51\n",
       "36   28663.76       127056.21        201126.82     Florida   90708.19\n",
       "37   44069.95        51283.14        197029.42  California   89949.14\n",
       "38   20229.59        65947.93        185265.10    New York   81229.06\n",
       "39   38558.51        82982.09        174999.30  California   81005.76\n",
       "40   28754.33       118546.05        172795.67  California   78239.91\n",
       "41   27892.92        84710.77        164470.71     Florida   77798.83\n",
       "42   23640.93        96189.63        148001.11  California   71498.49\n",
       "43   15505.73       127382.30         35534.17    New York   69758.98\n",
       "44   22177.74       154806.14         28334.72  California   65200.33\n",
       "45    1000.23       124153.04          1903.93    New York   64926.08\n",
       "46    1315.46       115816.21        297114.46     Florida   49490.75\n",
       "47       0.00       135426.92             0.00  California   42559.73\n",
       "48     542.05        51743.15             0.00    New York   35673.41\n",
       "49       0.00       116983.80         45173.06  California   14681.40"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "np.set_printoptions(suppress=True) #Otherwise prints in scientific format\n",
    "\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('50_Startups.csv')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Separate the dep and indep variables into a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, 4].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Encode Categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>165349.20</td>\n",
       "      <td>136897.80</td>\n",
       "      <td>471784.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162597.70</td>\n",
       "      <td>151377.59</td>\n",
       "      <td>443898.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>153441.51</td>\n",
       "      <td>101145.55</td>\n",
       "      <td>407934.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>144372.41</td>\n",
       "      <td>118671.85</td>\n",
       "      <td>383199.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>142107.34</td>\n",
       "      <td>91391.77</td>\n",
       "      <td>366168.42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2          3          4          5\n",
       "0  0.0  0.0  1.0  165349.20  136897.80  471784.10\n",
       "1  1.0  0.0  0.0  162597.70  151377.59  443898.53\n",
       "2  0.0  1.0  0.0  153441.51  101145.55  407934.54\n",
       "3  0.0  0.0  1.0  144372.41  118671.85  383199.62\n",
       "4  0.0  1.0  0.0  142107.34   91391.77  366168.42"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = dataset.iloc[:, :-1].values #Re running without this line will cause error\n",
    "# Encoding categorical data\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "X[:, 3] = labelencoder.fit_transform(X[:, 3])\n",
    "\n",
    "onehotencoder = OneHotEncoder(categorical_features = [3])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "pd.DataFrame(X)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Avoiding dummy variable trap\n",
    "\n",
    "Removing one dummy variable as it is redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>165349.20</td>\n",
       "      <td>136897.80</td>\n",
       "      <td>471784.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162597.70</td>\n",
       "      <td>151377.59</td>\n",
       "      <td>443898.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>153441.51</td>\n",
       "      <td>101145.55</td>\n",
       "      <td>407934.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>144372.41</td>\n",
       "      <td>118671.85</td>\n",
       "      <td>383199.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>142107.34</td>\n",
       "      <td>91391.77</td>\n",
       "      <td>366168.42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1          2          3          4\n",
       "0  0.0  1.0  165349.20  136897.80  471784.10\n",
       "1  0.0  0.0  162597.70  151377.59  443898.53\n",
       "2  1.0  0.0  153441.51  101145.55  407934.54\n",
       "3  0.0  1.0  144372.41  118671.85  383199.62\n",
       "4  1.0  0.0  142107.34   91391.77  366168.42"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = dataset.iloc[:, :-1].values #Re running without this line will cause error\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "X[:, 3] = labelencoder.fit_transform(X[:, 3])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [3])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "\n",
    "# Avoiding dummy variable trap\n",
    "X = X[:,1:]\n",
    "pd.DataFrame(X)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Spliting into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 10\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "print(len(X_train), len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Estimating MLR co-efficients from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta =  [-959.284  699.369    0.773    0.033    0.037]\n",
      "alpha =  42554.16761775606\n"
     ]
    }
   ],
   "source": [
    "# Same as Simple Linear Reg\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train,y_train)\n",
    "print(\"beta = \" , regressor.coef_)\n",
    "print(\"alpha = \", regressor.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Prediciting test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66051.52</td>\n",
       "      <td>182645.56</td>\n",
       "      <td>118148.20</td>\n",
       "      <td>103015.201598</td>\n",
       "      <td>103282.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100671.96</td>\n",
       "      <td>91790.61</td>\n",
       "      <td>249744.55</td>\n",
       "      <td>132582.277608</td>\n",
       "      <td>144259.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>101913.08</td>\n",
       "      <td>110594.11</td>\n",
       "      <td>229160.95</td>\n",
       "      <td>132447.738452</td>\n",
       "      <td>146121.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27892.92</td>\n",
       "      <td>84710.77</td>\n",
       "      <td>164470.71</td>\n",
       "      <td>71976.098513</td>\n",
       "      <td>77798.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>153441.51</td>\n",
       "      <td>101145.55</td>\n",
       "      <td>407934.54</td>\n",
       "      <td>178537.482211</td>\n",
       "      <td>191050.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>72107.60</td>\n",
       "      <td>127864.55</td>\n",
       "      <td>353183.81</td>\n",
       "      <td>116161.242302</td>\n",
       "      <td>105008.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20229.59</td>\n",
       "      <td>65947.93</td>\n",
       "      <td>185265.10</td>\n",
       "      <td>67851.692097</td>\n",
       "      <td>81229.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>61136.38</td>\n",
       "      <td>152701.92</td>\n",
       "      <td>88218.23</td>\n",
       "      <td>98791.733747</td>\n",
       "      <td>97483.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>73994.56</td>\n",
       "      <td>122782.75</td>\n",
       "      <td>303319.26</td>\n",
       "      <td>113969.435330</td>\n",
       "      <td>110352.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>142107.34</td>\n",
       "      <td>91391.77</td>\n",
       "      <td>366168.42</td>\n",
       "      <td>167921.065696</td>\n",
       "      <td>166187.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1          2          3          4              5          6\n",
       "0  1.0  0.0   66051.52  182645.56  118148.20  103015.201598  103282.38\n",
       "1  0.0  0.0  100671.96   91790.61  249744.55  132582.277608  144259.40\n",
       "2  1.0  0.0  101913.08  110594.11  229160.95  132447.738452  146121.95\n",
       "3  1.0  0.0   27892.92   84710.77  164470.71   71976.098513   77798.83\n",
       "4  1.0  0.0  153441.51  101145.55  407934.54  178537.482211  191050.39\n",
       "5  0.0  1.0   72107.60  127864.55  353183.81  116161.242302  105008.31\n",
       "6  0.0  1.0   20229.59   65947.93  185265.10   67851.692097   81229.06\n",
       "7  0.0  1.0   61136.38  152701.92   88218.23   98791.733747   97483.56\n",
       "8  1.0  0.0   73994.56  122782.75  303319.26  113969.435330  110352.25\n",
       "9  1.0  0.0  142107.34   91391.77  366168.42  167921.065696  166187.94"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "# Tablulating results\n",
    "res = np.column_stack((X_test,y_pred,y_test))\n",
    "pd.DataFrame(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Eliminating redundunt independent variables\n",
    "\n",
    "Are all independent variables statistically significant?\n",
    "\n",
    "We need the python library 'statsmodels'\n",
    "\n",
    "Installation in ubuntu : sudo pip3 install --upgrade --no-deps statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Appending a column of 1s to X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "statsmodels library needs $\\beta_{0}$ to be included in the vector $ {\\bf b}$ (see parameter 'exog' in sm.OLS). Hence the estimation should be of the form,\n",
    "\n",
    "$ y = {\\bf x.b} \\qquad x_{0} = 1, \\qquad {\\bf x,b \\in \\{0,1,2,...,m\\}} $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function append in module numpy.lib.function_base:\n",
      "\n",
      "append(arr, values, axis=None)\n",
      "    Append values to the end of an array.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    arr : array_like\n",
      "        Values are appended to a copy of this array.\n",
      "    values : array_like\n",
      "        These values are appended to a copy of `arr`.  It must be of the\n",
      "        correct shape (the same shape as `arr`, excluding `axis`).  If\n",
      "        `axis` is not specified, `values` can be any shape and will be\n",
      "        flattened before use.\n",
      "    axis : int, optional\n",
      "        The axis along which `values` are appended.  If `axis` is not\n",
      "        given, both `arr` and `values` are flattened before use.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    append : ndarray\n",
      "        A copy of `arr` with `values` appended to `axis`.  Note that\n",
      "        `append` does not occur in-place: a new array is allocated and\n",
      "        filled.  If `axis` is None, `out` is a flattened array.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    insert : Insert elements into an array.\n",
      "    delete : Delete elements from an array.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> np.append([1, 2, 3], [[4, 5, 6], [7, 8, 9]])\n",
      "    array([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "    \n",
      "    When `axis` is specified, `values` must have the correct shape.\n",
      "    \n",
      "    >>> np.append([[1, 2, 3], [4, 5, 6]], [[7, 8, 9]], axis=0)\n",
      "    array([[1, 2, 3],\n",
      "           [4, 5, 6],\n",
      "           [7, 8, 9]])\n",
      "    >>> np.append([[1, 2, 3], [4, 5, 6]], [7, 8, 9], axis=0)\n",
      "    Traceback (most recent call last):\n",
      "    ...\n",
      "    ValueError: arrays must have same number of dimensions\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(np.append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[     1.  ,      0.  ,      1.  , 165349.2 , 136897.8 , 471784.1 ],\n",
       "       [     1.  ,      0.  ,      0.  , 162597.7 , 151377.59, 443898.53],\n",
       "       [     1.  ,      1.  ,      0.  , 153441.51, 101145.55, 407934.54],\n",
       "       [     1.  ,      0.  ,      1.  , 144372.41, 118671.85, 383199.62],\n",
       "       [     1.  ,      1.  ,      0.  , 142107.34,  91391.77, 366168.42]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_b = np.append(arr = np.ones((len(X),1)), values = X, axis=1)\n",
    "X_b[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Filtering independent variables using statmodels library (Backward elimination)\n",
    "\n",
    "We need to create a new regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class OLS in module statsmodels.regression.linear_model:\n",
      "\n",
      "class OLS(WLS)\n",
      " |  A simple ordinary least squares model.\n",
      " |  \n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  endog : array-like\n",
      " |      1-d endogenous response variable. The dependent variable.\n",
      " |  exog : array-like\n",
      " |      A nobs x k array where `nobs` is the number of observations and `k`\n",
      " |      is the number of regressors. An intercept is not included by default\n",
      " |      and should be added by the user. See\n",
      " |      :func:`statsmodels.tools.add_constant`.\n",
      " |  missing : str\n",
      " |      Available options are 'none', 'drop', and 'raise'. If 'none', no nan\n",
      " |      checking is done. If 'drop', any observations with nans are dropped.\n",
      " |      If 'raise', an error is raised. Default is 'none.'\n",
      " |  hasconst : None or bool\n",
      " |      Indicates whether the RHS includes a user-supplied constant. If True,\n",
      " |      a constant is not checked for and k_constant is set to 1 and all\n",
      " |      result statistics are calculated as if a constant is present. If\n",
      " |      False, a constant is not checked for and k_constant is set to 0.\n",
      " |  \n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  weights : scalar\n",
      " |      Has an attribute weights = array(1.0) due to inheritance from WLS.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  GLS\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>>\n",
      " |  >>> import statsmodels.api as sm\n",
      " |  >>>\n",
      " |  >>> Y = [1,3,4,5,2,3,4]\n",
      " |  >>> X = range(1,8)\n",
      " |  >>> X = sm.add_constant(X)\n",
      " |  >>>\n",
      " |  >>> model = sm.OLS(Y,X)\n",
      " |  >>> results = model.fit()\n",
      " |  >>> results.params\n",
      " |  array([ 2.14285714,  0.25      ])\n",
      " |  >>> results.tvalues\n",
      " |  array([ 1.87867287,  0.98019606])\n",
      " |  >>> print(results.t_test([1, 0]))\n",
      " |  <T test: effect=array([ 2.14285714]), sd=array([[ 1.14062282]]), t=array([[ 1.87867287]]), p=array([[ 0.05953974]]), df_denom=5>\n",
      " |  >>> print(results.f_test(np.identity(2)))\n",
      " |  <F test: F=array([[ 19.46078431]]), p=[[ 0.00437251]], df_denom=5, df_num=2>\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  No constant is added by the model unless you are using formulas.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      OLS\n",
      " |      WLS\n",
      " |      RegressionModel\n",
      " |      statsmodels.base.model.LikelihoodModel\n",
      " |      statsmodels.base.model.Model\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, endog, exog=None, missing='none', hasconst=None, **kwargs)\n",
      " |      #TODO: change example to use datasets.  This was the point of datasets!\n",
      " |  \n",
      " |  fit_regularized(self, method='elastic_net', alpha=0.0, L1_wt=1.0, start_params=None, profile_scale=False, refit=False, **kwargs)\n",
      " |      Return a regularized fit to a linear regression model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      method : string\n",
      " |          Only the 'elastic_net' approach is currently implemented.\n",
      " |      alpha : scalar or array-like\n",
      " |          The penalty weight.  If a scalar, the same penalty weight\n",
      " |          applies to all variables in the model.  If a vector, it\n",
      " |          must have the same length as `params`, and contains a\n",
      " |          penalty weight for each coefficient.\n",
      " |      L1_wt: scalar\n",
      " |          The fraction of the penalty given to the L1 penalty term.\n",
      " |          Must be between 0 and 1 (inclusive).  If 0, the fit is a\n",
      " |          ridge fit, if 1 it is a lasso fit.\n",
      " |      start_params : array-like\n",
      " |          Starting values for ``params``.\n",
      " |      profile_scale : bool\n",
      " |          If True the penalized fit is computed using the profile\n",
      " |          (concentrated) log-likelihood for the Gaussian model.\n",
      " |          Otherwise the fit uses the residual sum of squares.\n",
      " |      refit : bool\n",
      " |          If True, the model is refit using only the variables that\n",
      " |          have non-zero coefficients in the regularized fit.  The\n",
      " |          refitted model is not regularized.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      An array of coefficients, or a RegressionResults object of the\n",
      " |      same type returned by ``fit``.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      \n",
      " |      The elastic net approach closely follows that implemented in\n",
      " |      the glmnet package in R.  The penalty is a combination of L1\n",
      " |      and L2 penalties.\n",
      " |      \n",
      " |      The function that is minimized is: ..math::\n",
      " |      \n",
      " |          0.5*RSS/n + alpha*((1-L1_wt)*|params|_2^2/2 + L1_wt*|params|_1)\n",
      " |      \n",
      " |      where RSS is the usual regression sum of squares, n is the\n",
      " |      sample size, and :math:`|*|_1` and :math:`|*|_2` are the L1 and L2\n",
      " |      norms.\n",
      " |      \n",
      " |      Post-estimation results are based on the same data used to\n",
      " |      select variables, hence may be subject to overfitting biases.\n",
      " |      \n",
      " |      The elastic_net method uses the following keyword arguments:\n",
      " |      \n",
      " |      maxiter : int\n",
      " |          Maximum number of iterations\n",
      " |      cnvrg_tol : float\n",
      " |          Convergence threshold for line searches\n",
      " |      zero_tol : float\n",
      " |          Coefficients below this threshold are treated as zero.\n",
      " |      \n",
      " |      References\n",
      " |      ----------\n",
      " |      Friedman, Hastie, Tibshirani (2008).  Regularization paths for\n",
      " |      generalized linear models via coordinate descent.  Journal of\n",
      " |      Statistical Software 33(1), 1-22 Feb 2010.\n",
      " |  \n",
      " |  hessian(self, params, scale=None)\n",
      " |      Evaluate the Hessian function at a given point.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : array-like\n",
      " |          The parameter vector at which the Hessian is computed.\n",
      " |      scale : float or None\n",
      " |          If None, return the profile (concentrated) log likelihood\n",
      " |          (profiled over the scale parameter), else return the\n",
      " |          log-likelihood using the given scale value.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      The Hessian matrix.\n",
      " |  \n",
      " |  loglike(self, params, scale=None)\n",
      " |      The likelihood function for the OLS model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : array-like\n",
      " |          The coefficients with which to estimate the log-likelihood.\n",
      " |      scale : float or None\n",
      " |          If None, return the profile (concentrated) log likelihood\n",
      " |          (profiled over the scale parameter), else return the\n",
      " |          log-likelihood using the given scale value.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      The likelihood function evaluated at params.\n",
      " |  \n",
      " |  score(self, params, scale=None)\n",
      " |      Evaluate the score function at a given point.\n",
      " |      \n",
      " |      The score corresponds to the profile (concentrated)\n",
      " |      log-likelihood in which the scale parameter has been profiled\n",
      " |      out.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : array-like\n",
      " |          The parameter vector at which the score function is\n",
      " |          computed.\n",
      " |      scale : float or None\n",
      " |          If None, return the profile (concentrated) log likelihood\n",
      " |          (profiled over the scale parameter), else return the\n",
      " |          log-likelihood using the given scale value.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      The score vector.\n",
      " |  \n",
      " |  whiten(self, Y)\n",
      " |      OLS model whitener does nothing: returns Y.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from RegressionModel:\n",
      " |  \n",
      " |  fit(self, method='pinv', cov_type='nonrobust', cov_kwds=None, use_t=None, **kwargs)\n",
      " |      Full fit of the model.\n",
      " |      \n",
      " |      The results include an estimate of covariance matrix, (whitened)\n",
      " |      residuals and an estimate of scale.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      method : str, optional\n",
      " |          Can be \"pinv\", \"qr\".  \"pinv\" uses the Moore-Penrose pseudoinverse\n",
      " |          to solve the least squares problem. \"qr\" uses the QR\n",
      " |          factorization.\n",
      " |      cov_type : str, optional\n",
      " |          See `regression.linear_model.RegressionResults` for a description\n",
      " |          of the available covariance estimators\n",
      " |      cov_kwds : list or None, optional\n",
      " |          See `linear_model.RegressionResults.get_robustcov_results` for a\n",
      " |          description required keywords for alternative covariance estimators\n",
      " |      use_t : bool, optional\n",
      " |          Flag indicating to use the Student's t distribution when computing\n",
      " |          p-values.  Default behavior depends on cov_type. See\n",
      " |          `linear_model.RegressionResults.get_robustcov_results` for\n",
      " |          implementation details.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      A RegressionResults class instance.\n",
      " |      \n",
      " |      See Also\n",
      " |      ---------\n",
      " |      regression.linear_model.RegressionResults\n",
      " |      regression.linear_model.RegressionResults.get_robustcov_results\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The fit method uses the pseudoinverse of the design/exogenous variables\n",
      " |      to solve the least squares minimization.\n",
      " |  \n",
      " |  get_distribution(self, params, scale, exog=None, dist_class=None)\n",
      " |      Returns a random number generator for the predictive distribution.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : array-like\n",
      " |          The model parameters (regression coefficients).\n",
      " |      scale : scalar\n",
      " |          The variance parameter.\n",
      " |      exog : array-like\n",
      " |          The predictor variable matrix.\n",
      " |      dist_class : class\n",
      " |          A random number generator class.  Must take 'loc' and\n",
      " |          'scale' as arguments and return a random number generator\n",
      " |          implementing an `rvs` method for simulating random values.\n",
      " |          Defaults to Gaussian.\n",
      " |      \n",
      " |      Returns a frozen random number generator object with mean and\n",
      " |      variance determined by the fitted linear model.  Use the\n",
      " |      ``rvs`` method to generate random values.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Due to the behavior of ``scipy.stats.distributions objects``,\n",
      " |      the returned random number generator must be called with\n",
      " |      ``gen.rvs(n)`` where ``n`` is the number of observations in\n",
      " |      the data set used to fit the model.  If any other value is\n",
      " |      used for ``n``, misleading results will be produced.\n",
      " |  \n",
      " |  initialize(self)\n",
      " |  \n",
      " |  predict(self, params, exog=None)\n",
      " |      Return linear predicted values from a design matrix.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : array-like\n",
      " |          Parameters of a linear model\n",
      " |      exog : array-like, optional.\n",
      " |          Design / exogenous data. Model exog is used if None.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      An array of fitted values\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      If the model has not yet been fit, params is not optional.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from RegressionModel:\n",
      " |  \n",
      " |  df_model\n",
      " |      The model degree of freedom, defined as the rank of the regressor\n",
      " |      matrix minus 1 if a constant is included.\n",
      " |  \n",
      " |  df_resid\n",
      " |      The residual degree of freedom, defined as the number of observations\n",
      " |      minus the rank of the regressor matrix.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from statsmodels.base.model.LikelihoodModel:\n",
      " |  \n",
      " |  information(self, params)\n",
      " |      Fisher information matrix of model\n",
      " |      \n",
      " |      Returns -Hessian of loglike evaluated at params.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from statsmodels.base.model.Model:\n",
      " |  \n",
      " |  from_formula(formula, data, subset=None, drop_cols=None, *args, **kwargs) from builtins.type\n",
      " |      Create a Model from a formula and dataframe.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      formula : str or generic Formula object\n",
      " |          The formula specifying the model\n",
      " |      data : array-like\n",
      " |          The data for the model. See Notes.\n",
      " |      subset : array-like\n",
      " |          An array-like object of booleans, integers, or index values that\n",
      " |          indicate the subset of df to use in the model. Assumes df is a\n",
      " |          `pandas.DataFrame`\n",
      " |      drop_cols : array-like\n",
      " |          Columns to drop from the design matrix.  Cannot be used to\n",
      " |          drop terms involving categoricals.\n",
      " |      args : extra arguments\n",
      " |          These are passed to the model\n",
      " |      kwargs : extra keyword arguments\n",
      " |          These are passed to the model with one exception. The\n",
      " |          ``eval_env`` keyword is passed to patsy. It can be either a\n",
      " |          :class:`patsy:patsy.EvalEnvironment` object or an integer\n",
      " |          indicating the depth of the namespace to use. For example, the\n",
      " |          default ``eval_env=0`` uses the calling namespace. If you wish\n",
      " |          to use a \"clean\" environment set ``eval_env=-1``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      model : Model instance\n",
      " |      \n",
      " |      Notes\n",
      " |      ------\n",
      " |      data must define __getitem__ with the keys in the formula terms\n",
      " |      args and kwargs are passed on to the model instantiation. E.g.,\n",
      " |      a numpy structured or rec array, a dictionary, or a pandas DataFrame.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from statsmodels.base.model.Model:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  endog_names\n",
      " |      Names of endogenous variables\n",
      " |  \n",
      " |  exog_names\n",
      " |      Names of exogenous variables\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sm.OLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new regressor object\n",
    "X_opt = X_b[:,[0,1,2,3,4,5]]\n",
    "regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b1) Identify the variable with highest p-value, remove that variable if its p-value is greater than certain significant level (say 0.06). Refit and repeat this process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.951</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.945</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   169.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 02 Feb 2018</td> <th>  Prob (F-statistic):</th> <td>1.34e-27</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:19:36</td>     <th>  Log-Likelihood:    </th> <td> -525.38</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1063.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    44</td>      <th>  BIC:               </th> <td>   1074.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td> 5.013e+04</td> <td> 6884.820</td> <td>    7.281</td> <td> 0.000</td> <td> 3.62e+04</td> <td>  6.4e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>  198.7888</td> <td> 3371.007</td> <td>    0.059</td> <td> 0.953</td> <td>-6595.030</td> <td> 6992.607</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>  -41.8870</td> <td> 3256.039</td> <td>   -0.013</td> <td> 0.990</td> <td>-6604.003</td> <td> 6520.229</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>    0.8060</td> <td>    0.046</td> <td>   17.369</td> <td> 0.000</td> <td>    0.712</td> <td>    0.900</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>   -0.0270</td> <td>    0.052</td> <td>   -0.517</td> <td> 0.608</td> <td>   -0.132</td> <td>    0.078</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>    0.0270</td> <td>    0.017</td> <td>    1.574</td> <td> 0.123</td> <td>   -0.008</td> <td>    0.062</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>14.782</td> <th>  Durbin-Watson:     </th> <td>   1.283</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  21.266</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.948</td> <th>  Prob(JB):          </th> <td>2.41e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.572</td> <th>  Cond. No.          </th> <td>1.45e+06</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.951\n",
       "Model:                            OLS   Adj. R-squared:                  0.945\n",
       "Method:                 Least Squares   F-statistic:                     169.9\n",
       "Date:                Fri, 02 Feb 2018   Prob (F-statistic):           1.34e-27\n",
       "Time:                        16:19:36   Log-Likelihood:                -525.38\n",
       "No. Observations:                  50   AIC:                             1063.\n",
       "Df Residuals:                      44   BIC:                             1074.\n",
       "Df Model:                           5                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       5.013e+04   6884.820      7.281      0.000    3.62e+04     6.4e+04\n",
       "x1           198.7888   3371.007      0.059      0.953   -6595.030    6992.607\n",
       "x2           -41.8870   3256.039     -0.013      0.990   -6604.003    6520.229\n",
       "x3             0.8060      0.046     17.369      0.000       0.712       0.900\n",
       "x4            -0.0270      0.052     -0.517      0.608      -0.132       0.078\n",
       "x5             0.0270      0.017      1.574      0.123      -0.008       0.062\n",
       "==============================================================================\n",
       "Omnibus:                       14.782   Durbin-Watson:                   1.283\n",
       "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               21.266\n",
       "Skew:                          -0.948   Prob(JB):                     2.41e-05\n",
       "Kurtosis:                       5.572   Cond. No.                     1.45e+06\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.45e+06. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor_OLS.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b2) x2 has the highest p-value. Hence remove x2 and create a new regressor object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.951</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.946</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   217.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 02 Feb 2018</td> <th>  Prob (F-statistic):</th> <td>8.49e-29</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:19:36</td>     <th>  Log-Likelihood:    </th> <td> -525.38</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   1061.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    45</td>      <th>  BIC:               </th> <td>   1070.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td> 5.011e+04</td> <td> 6647.870</td> <td>    7.537</td> <td> 0.000</td> <td> 3.67e+04</td> <td> 6.35e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>  220.1585</td> <td> 2900.536</td> <td>    0.076</td> <td> 0.940</td> <td>-5621.821</td> <td> 6062.138</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    0.8060</td> <td>    0.046</td> <td>   17.606</td> <td> 0.000</td> <td>    0.714</td> <td>    0.898</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>   -0.0270</td> <td>    0.052</td> <td>   -0.523</td> <td> 0.604</td> <td>   -0.131</td> <td>    0.077</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>    0.0270</td> <td>    0.017</td> <td>    1.592</td> <td> 0.118</td> <td>   -0.007</td> <td>    0.061</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>14.758</td> <th>  Durbin-Watson:     </th> <td>   1.282</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.001</td> <th>  Jarque-Bera (JB):  </th> <td>  21.172</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.948</td> <th>  Prob(JB):          </th> <td>2.53e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.563</td> <th>  Cond. No.          </th> <td>1.40e+06</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.951\n",
       "Model:                            OLS   Adj. R-squared:                  0.946\n",
       "Method:                 Least Squares   F-statistic:                     217.2\n",
       "Date:                Fri, 02 Feb 2018   Prob (F-statistic):           8.49e-29\n",
       "Time:                        16:19:36   Log-Likelihood:                -525.38\n",
       "No. Observations:                  50   AIC:                             1061.\n",
       "Df Residuals:                      45   BIC:                             1070.\n",
       "Df Model:                           4                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       5.011e+04   6647.870      7.537      0.000    3.67e+04    6.35e+04\n",
       "x1           220.1585   2900.536      0.076      0.940   -5621.821    6062.138\n",
       "x2             0.8060      0.046     17.606      0.000       0.714       0.898\n",
       "x3            -0.0270      0.052     -0.523      0.604      -0.131       0.077\n",
       "x4             0.0270      0.017      1.592      0.118      -0.007       0.061\n",
       "==============================================================================\n",
       "Omnibus:                       14.758   Durbin-Watson:                   1.282\n",
       "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               21.172\n",
       "Skew:                          -0.948   Prob(JB):                     2.53e-05\n",
       "Kurtosis:                       5.563   Cond. No.                     1.40e+06\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.4e+06. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a new regressor object\n",
    "X_opt = X_b[:,[0,1,3,4,5]]\n",
    "regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n",
    "regressor_OLS.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b3) Remove x1 and refit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.   , 0.   , 0.602, 0.105])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_opt = X_b[:,[0,3,4,5]]\n",
    "regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n",
    "regressor_OLS.pvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b3) Remove x4 and refit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.  , 0.06])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_opt = X_b[:,[0,3,5]]\n",
    "regressor_OLS = sm.OLS(endog = y, exog = X_opt).fit()\n",
    "regressor_OLS.pvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b4)** Now all the variables have p-values less than the significant levels. We can see that only **R&D expenditure** and **Market spends** are the statistically significant factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: \n",
    "\n",
    "Ideally, **Test-Performace** must be compared after every elimination. Performance metrics will be discussed in later sections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
